"""
æœ¬åœ°æ•°æ®åŒæ­¥æœåŠ¡
åŸºäºŽ Tushare Pro + SQLite å­˜å‚¨

ä¼˜åŠ¿ï¼š
- å•æ–‡ä»¶å­˜å‚¨ï¼Œä¾¿äºŽå¤‡ä»½è¿ç§»
- æ”¯æŒ SQL æŸ¥è¯¢ï¼Œç­›é€‰çµæ´»
- æ‰¹é‡å†™å…¥é«˜æ•ˆ
- Python å†…ç½®æ”¯æŒ
"""
import os
import json
import sqlite3
import tushare as ts
import pandas as pd
from datetime import datetime, timedelta
from typing import Optional, List, Dict
import time
import threading
import fcntl

from config import TUSHARE_TOKEN


# å…¨å±€çº¿ç¨‹é”
_sync_lock = threading.Lock()


class DataSyncService:
    """
    æœ¬åœ°æ•°æ®åŒæ­¥æœåŠ¡ï¼ˆSQLite å­˜å‚¨ç‰ˆï¼‰
    
    å­˜å‚¨ç»“æž„ï¼š
        data/
        â”œâ”€â”€ stocks.db          # SQLite æ•°æ®åº“
        â””â”€â”€ metadata.json      # åŒæ­¥å…ƒæ•°æ®
    
    æ•°æ®è¡¨ï¼š
        daily_data: æ—¥æœŸ, ä»£ç , å¼€ç›˜, æœ€é«˜, æœ€ä½Ž, æ”¶ç›˜, æ¶¨è·Œå¹…, æˆäº¤é‡, æ¢æ‰‹çŽ‡, PE, PB, å¸‚å€¼...
    """
    
    _is_syncing = False
    _sync_start_time = None
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.db_path = os.path.join(data_dir, "stocks.db")
        self.metadata_path = os.path.join(data_dir, "metadata.json")
        self.lock_file_path = os.path.join(data_dir, ".sync.lock")
        self._stop_requested = False
        self._lock_fd = None
        
        # åˆå§‹åŒ– Tushare Pro
        if not TUSHARE_TOKEN:
            raise ValueError("Tushare Token æœªé…ç½®")
        ts.set_token(TUSHARE_TOKEN)
        self.pro = ts.pro_api()
        
        # åˆ›å»ºç›®å½•å’Œæ•°æ®åº“
        os.makedirs(data_dir, exist_ok=True)
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æž„"""
        with sqlite3.connect(self.db_path) as conn:
            # 1. æ—¥çº¿æ•°æ®è¡¨
            conn.execute('''
                CREATE TABLE IF NOT EXISTS daily_data (
                    æ—¥æœŸ TEXT NOT NULL,
                    ä»£ç  TEXT NOT NULL,
                    åç§° TEXT,
                    å¼€ç›˜ REAL,
                    æœ€é«˜ REAL,
                    æœ€ä½Ž REAL,
                    æ”¶ç›˜ REAL,
                    æ˜¨æ”¶ REAL,
                    æ¶¨è·Œé¢ REAL,
                    æ¶¨è·Œå¹… REAL,
                    æˆäº¤é‡ REAL,
                    æˆäº¤é¢ REAL,
                    æ¢æ‰‹çŽ‡ REAL,
                    é‡æ¯” REAL,
                    PE REAL,
                    PE_TTM REAL,
                    PB REAL,
                    æ€»å¸‚å€¼ REAL,
                    æµé€šå¸‚å€¼ REAL,
                    æ€»è‚¡æœ¬ REAL,
                    æµé€šè‚¡æœ¬ REAL,
                    å¤æƒå› å­ REAL,
                    qfq_å¼€ç›˜ REAL,
                    qfq_æœ€é«˜ REAL,
                    qfq_æœ€ä½Ž REAL,
                    qfq_æ”¶ç›˜ REAL,
                    ma5 REAL,
                    ma10 REAL,
                    ma20 REAL,
                    vma5 REAL,
                    vma10 REAL,
                    vma20 REAL,
                    PRIMARY KEY (æ—¥æœŸ, ä»£ç )
                )
            ''')
            
            # 2. è‚¡ç¥¨åŸºç¡€ä¿¡æ¯è¡¨ (ç”¨äºŽåç§°å…³è”)
            conn.execute('''
                CREATE TABLE IF NOT EXISTS stock_basic (
                    ä»£ç  TEXT PRIMARY KEY,
                    åç§° TEXT,
                    è¡Œä¸š TEXT,
                    åŒºåŸŸ TEXT,
                    å¸‚åœº TEXT,
                    ä¸Šå¸‚æ—¥æœŸ TEXT
                )
            ''')

            # 3. æ”¶è—è‚¡ç¥¨è¡¨
            conn.execute('''
                CREATE TABLE IF NOT EXISTS collected_stocks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    ä»£ç  TEXT NOT NULL,
                    åç§° TEXT,
                    æ”¶è—æ—¥æœŸ TEXT NOT NULL,
                    ç­–ç•¥åç§° TEXT NOT NULL,
                    å¤‡æ³¨ TEXT,
                    UNIQUE(ä»£ç , ç­–ç•¥åç§°)
                )
            ''')
            
            # åˆ›å»ºç´¢å¼•
            conn.execute('CREATE INDEX IF NOT EXISTS idx_daily_date ON daily_data (æ—¥æœŸ)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_daily_code ON daily_data (ä»£ç )')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_pct_chg ON daily_data(æ¶¨è·Œå¹…)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_collect_code ON collected_stocks (ä»£ç )')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_collect_strategy ON collected_stocks (ç­–ç•¥åç§°)')
            
            # åŠ¨æ€æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„åˆ—ï¼ˆé€‚é…å·²æœ‰æ•°æ®åº“å‡çº§ï¼‰
            cursor = conn.execute("PRAGMA table_info(daily_data)")
            existing_cols = {row[1] for row in cursor.fetchall()}
            
            # éœ€è¦å­˜åœ¨çš„åˆ—
            target_cols = {
                'åç§°': 'TEXT',
                'å¤æƒå› å­': 'REAL',
                'qfq_å¼€ç›˜': 'REAL',
                'qfq_æœ€é«˜': 'REAL',
                'qfq_æœ€ä½Ž': 'REAL',
                'qfq_æ”¶ç›˜': 'REAL',
                'ma5': 'REAL',
                'ma10': 'REAL',
                'ma20': 'REAL',
                'ma60': 'REAL',
                'vma5': 'REAL',
                'vma10': 'REAL',
                'vma20': 'REAL'
            }
            
            for col, col_type in target_cols.items():
                if col not in existing_cols:
                    print(f"ðŸ”§ æ­£åœ¨å‡çº§æ•°æ®åº“ï¼šæ·»åŠ åˆ— {col}...")
                    try:
                        conn.execute(f"ALTER TABLE daily_data ADD COLUMN {col} {col_type}")
                    except Exception as e:
                        print(f"âš ï¸ æ·»åŠ åˆ— {col} å¤±è´¥: {e}")
            
            conn.commit()
    
    # ==================== é”æœºåˆ¶ ====================
    
    def _acquire_lock(self) -> bool:
        if not _sync_lock.acquire(blocking=False):
            return False
        try:
            self._lock_fd = open(self.lock_file_path, 'w')
            fcntl.flock(self._lock_fd.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
            lock_info = {"pid": os.getpid(), "start_time": datetime.now().isoformat()}
            self._lock_fd.write(json.dumps(lock_info))
            self._lock_fd.flush()
            DataSyncService._is_syncing = True
            DataSyncService._sync_start_time = datetime.now()
            return True
        except (IOError, OSError):
            _sync_lock.release()
            return False
    
    def _release_lock(self):
        DataSyncService._is_syncing = False
        DataSyncService._sync_start_time = None
        if self._lock_fd:
            try:
                fcntl.flock(self._lock_fd.fileno(), fcntl.LOCK_UN)
                self._lock_fd.close()
                if os.path.exists(self.lock_file_path):
                    os.remove(self.lock_file_path)
            except:
                pass
            self._lock_fd = None
        try:
            _sync_lock.release()
        except:
            pass
    
    def is_syncing(self) -> bool:
        if DataSyncService._is_syncing:
            return True
        if os.path.exists(self.lock_file_path):
            try:
                with open(self.lock_file_path, 'r') as f:
                    lock_info = json.load(f)
                    pid = lock_info.get("pid")
                    if pid:
                        try:
                            os.kill(pid, 0)
                            return True
                        except OSError:
                            pass
            except:
                pass
        return False
    
    def get_sync_status(self) -> dict:
        if self.is_syncing():
            start_time = DataSyncService._sync_start_time
            elapsed = (datetime.now() - start_time).total_seconds() if start_time else 0
            return {"is_syncing": True, "elapsed_seconds": int(elapsed)}
        return {"is_syncing": False, "elapsed_seconds": 0}
    
    def request_stop(self):
        self._stop_requested = True
    
    # ==================== å…ƒæ•°æ® ====================
    
    def get_metadata(self) -> dict:
        if os.path.exists(self.metadata_path):
            with open(self.metadata_path, 'r') as f:
                return json.load(f)
        return {"last_sync_date": None, "total_stocks": 0, "days": 0}
    
    def save_metadata(self, metadata: dict):
        with open(self.metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    # ==================== äº¤æ˜“æ—¥åŽ† ====================
    
    def recompute_technical_indicators(self):
        """é‡æ–°è®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„å‰å¤æƒä»·æ ¼å’Œå‡çº¿æŒ‡æ ‡"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # 1. åŠ è½½æ‰€æœ‰è¡Œæƒ…æ•°æ®
                print("   åŠ è½½æ•°æ®...")
                df = pd.read_sql_query("SELECT * FROM daily_data ORDER BY ä»£ç , æ—¥æœŸ", conn)
                
                if df.empty:
                    return
                
                print(f"   è®¡ç®—æŒ‡æ ‡ ({len(df)} æ¡è®°å½•)...")
                # ç¡®ä¿ä»£ç é¡ºåº
                df = df.sort_values(['ä»£ç ', 'æ—¥æœŸ'])
                
                # è®¡ç®—å‰å¤æƒæ”¶ç›˜ä»·
                # å‰å¤æƒåŽŸç†ï¼šP' = P * (Current_Factor / Last_Factor)
                
                # èŽ·å–æ¯åªè‚¡ç¥¨æœ€åŽä¸€æ¬¡æœ‰æ•ˆçš„å¤æƒå› å­ä½œä¸ºåŸºå‡†
                # å¿…é¡»å…ˆå‰”é™¤ NULL æ‰èƒ½æ‰¾åˆ°çœŸæ­£çš„â€œæœ€åŽâ€ä¸€ä¸ªå› å­
                valid_factors = df[df['å¤æƒå› å­'].notnull()].groupby('ä»£ç ')['å¤æƒå› å­'].last()
                df['base_factor'] = df['ä»£ç '].map(valid_factors)
                
                # åªæœ‰å½“åŽŸå§‹ä»·æ ¼å’Œå½“å‰å¤æƒå› å­éƒ½å­˜åœ¨æ—¶æ‰è®¡ç®—
                mask = df['æ”¶ç›˜'].notnull() & df['å¤æƒå› å­'].notnull() & df['base_factor'].notnull()
                
                # æ¯”ä¾‹ç³»æ•°
                df['adj_ratio'] = 1.0
                df.loc[mask, 'adj_ratio'] = df.loc[mask, 'å¤æƒå› å­'] / df.loc[mask, 'base_factor']
                
                # å‰å¤æƒ OHLC
                df['qfq_å¼€ç›˜'] = (df['å¼€ç›˜'] * df['adj_ratio']).round(2)
                df['qfq_æœ€é«˜'] = (df['æœ€é«˜'] * df['adj_ratio']).round(2)
                df['qfq_æœ€ä½Ž'] = (df['æœ€ä½Ž'] * df['adj_ratio']).round(2)
                df['qfq_æ”¶ç›˜'] = (df['æ”¶ç›˜'] * df['adj_ratio']).round(2)
                
                # å¯¹äºŽè¿˜æ˜¯ NULL çš„ï¼ˆæ¯”å¦‚æ²¡æœ‰å¤æƒå› å­çš„å“ç§ï¼‰ï¼Œä½¿ç”¨åŽŸå§‹ä»·æ ¼
                df['qfq_æ”¶ç›˜'] = df['qfq_æ”¶ç›˜'].fillna(df['æ”¶ç›˜'])
                df['qfq_å¼€ç›˜'] = df['qfq_å¼€ç›˜'].fillna(df['å¼€ç›˜'])
                df['qfq_æœ€é«˜'] = df['qfq_æœ€é«˜'].fillna(df['æœ€é«˜'])
                df['qfq_æœ€ä½Ž'] = df['qfq_æœ€ä½Ž'].fillna(df['æœ€ä½Ž'])
                
                # è®¡ç®—å‡çº¿ (åŸºäºŽå‰å¤æƒæ”¶ç›˜ä»·)
                gp = df.groupby('ä»£ç ')['qfq_æ”¶ç›˜']
                df['ma5'] = gp.transform(lambda x: x.rolling(5).mean()).round(2)
                df['ma10'] = gp.transform(lambda x: x.rolling(10).mean()).round(2)
                df['ma20'] = gp.transform(lambda x: x.rolling(20).mean()).round(2)
                df['ma60'] = gp.transform(lambda x: x.rolling(60).mean()).round(2)
                
                # è®¡ç®—å‡é‡
                gv = df.groupby('ä»£ç ')['æˆäº¤é‡']
                df['vma5'] = gv.transform(lambda x: x.rolling(5).mean()).round(0)
                df['vma10'] = gv.transform(lambda x: x.rolling(10).mean()).round(0)
                df['vma20'] = gv.transform(lambda x: x.rolling(20).mean()).round(0)
                
                # 2. å›žå†™æ•°æ®åº“
                print("   ä¿å­˜ç»“æžœ...")
                update_cols = ['qfq_å¼€ç›˜', 'qfq_æœ€é«˜', 'qfq_æœ€ä½Ž', 'qfq_æ”¶ç›˜', 'ma5', 'ma10', 'ma20', 'ma60', 'vma5', 'vma10', 'vma20', 'æ—¥æœŸ', 'ä»£ç ']
                df_update = df[update_cols]
                
                # ä½¿ç”¨äº‹åŠ¡æ‰¹é‡æ›´æ–°
                cursor = conn.cursor()
                sql = '''
                    UPDATE daily_data 
                    SET qfq_å¼€ç›˜ = ?, qfq_æœ€é«˜ = ?, qfq_æœ€ä½Ž = ?, qfq_æ”¶ç›˜ = ?, 
                        ma5 = ?, ma10 = ?, ma20 = ?, ma60 = ?, vma5 = ?, vma10 = ?, vma20 = ?
                    WHERE æ—¥æœŸ = ? AND ä»£ç  = ?
                '''
                data = [tuple(x) for x in df_update.values]
                cursor.executemany(sql, data)
                conn.commit()
                print("âœ… æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆ")
                
        except Exception as e:
            print(f"âŒ è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    # ==================== è‚¡ç¥¨åˆ—è¡¨ä¸ŽåŸºç¡€ä¿¡æ¯ ====================
    
    def sync_stock_basic(self) -> bool:
        """åŒæ­¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ (åç§°ã€è¡Œä¸šç­‰)"""
        try:
            print("æ­£åœ¨ä»Ž Tushare èŽ·å–å…¨å¸‚åœºè‚¡ç¥¨åŸºç¡€ä¿¡æ¯...")
            df = self.pro.stock_basic(
                list_status='L',
                fields='symbol,name,area,industry,market,list_date,delist_date,list_status'
            )
            
            if df is None or df.empty:
                return False
                
            df = df.rename(columns={
                'symbol': 'ä»£ç ',
                'name': 'åç§°',
                'area': 'åœ°åŒº',
                'industry': 'è¡Œä¸š',
                'market': 'å¸‚åœº',
                'list_date': 'ä¸Šå¸‚æ—¥æœŸ',
                'delist_date': 'é€€å¸‚æ—¥æœŸ',
                'list_status': 'çŠ¶æ€'
            })
            
            with sqlite3.connect(self.db_path) as conn:
                df.to_sql('stock_basic', conn, if_exists='replace', index=False)
                conn.commit()
            
            print(f"âœ… è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥å®Œæˆï¼Œå…± {len(df)} åªè‚¡ç¥¨")
            return True
        except Exception as e:
            print(f"âŒ åŒæ­¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯å¤±è´¥: {e}")
            return False

    def get_trading_days(self, start_date: str, end_date: str) -> List[str]:
        """èŽ·å–äº¤æ˜“æ—¥åˆ—è¡¨"""
        try:
            df = self.pro.trade_cal(
                exchange='SSE',
                start_date=start_date,
                end_date=end_date,
                is_open='1'
            )
            return sorted(df['cal_date'].tolist())
        except Exception as e:
            print(f"èŽ·å–äº¤æ˜“æ—¥åŽ†å¤±è´¥: {e}")
            return []
    
    # ==================== æŒ‰æ—¥æœŸæ‰¹é‡èŽ·å– ====================
    
    def fetch_daily_by_date(self, trade_date: str) -> pd.DataFrame:
        """æŒ‰æ—¥æœŸèŽ·å–å…¨å¸‚åœºæ—¥çº¿è¡Œæƒ…"""
        try:
            df = self.pro.daily(trade_date=trade_date)
            if df is not None and not df.empty:
                df['ä»£ç '] = df['ts_code'].str[:6]
                df = df.rename(columns={
                    'trade_date': 'æ—¥æœŸ',
                    'open': 'å¼€ç›˜', 'high': 'æœ€é«˜', 'low': 'æœ€ä½Ž',
                    'close': 'æ”¶ç›˜', 'pre_close': 'æ˜¨æ”¶',
                    'change': 'æ¶¨è·Œé¢', 'pct_chg': 'æ¶¨è·Œå¹…',
                    'vol': 'æˆäº¤é‡', 'amount': 'æˆäº¤é¢'
                })
                df = df.drop(columns=['ts_code'], errors='ignore')
            return df
        except Exception as e:
            print(f"èŽ·å– {trade_date} daily å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def fetch_daily_basic_by_date(self, trade_date: str) -> pd.DataFrame:
        """æŒ‰æ—¥æœŸèŽ·å–å…¨å¸‚åœºæ¯æ—¥æŒ‡æ ‡"""
        try:
            df = self.pro.daily_basic(trade_date=trade_date)
            if df is not None and not df.empty:
                df['ä»£ç '] = df['ts_code'].str[:6]
                df = df.rename(columns={
                    'trade_date': 'æ—¥æœŸ',
                    'turnover_rate': 'æ¢æ‰‹çŽ‡',
                    'volume_ratio': 'é‡æ¯”',
                    'pe': 'PE', 'pe_ttm': 'PE_TTM', 'pb': 'PB',
                    'total_share': 'æ€»è‚¡æœ¬', 'float_share': 'æµé€šè‚¡æœ¬',
                    'total_mv': 'æ€»å¸‚å€¼', 'circ_mv': 'æµé€šå¸‚å€¼'
                })
                keep = ['ä»£ç ', 'æ—¥æœŸ', 'æ¢æ‰‹çŽ‡', 'é‡æ¯”', 'PE', 'PE_TTM', 'PB',
                       'æ€»å¸‚å€¼', 'æµé€šå¸‚å€¼', 'æ€»è‚¡æœ¬', 'æµé€šè‚¡æœ¬']
                available = [c for c in keep if c in df.columns]
                df = df[available]
            return df
        except Exception as e:
            print(f"èŽ·å– {trade_date} daily_basic å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def fetch_and_merge_by_date(self, trade_date: str) -> pd.DataFrame:
        """èŽ·å–å¹¶åˆå¹¶æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰è‚¡ç¥¨è¡Œæƒ…æŒ‡æ ‡"""
        try:
            # 1. åŸºç¡€æ—¥çº¿
            df_daily = self.fetch_daily_by_date(trade_date)
            # 2. æ¯æ—¥æŒ‡æ ‡ (PE/PBç­‰)
            df_basic = self.fetch_daily_basic_by_date(trade_date)
            # 3. å¤æƒå› å­
            df_adj = self.fetch_adj_factor_by_date(trade_date)
            
            if df_daily.empty:
                return pd.DataFrame()
            
            # åˆå¹¶
            df = df_daily.merge(df_basic, on=['ä»£ç ', 'æ—¥æœŸ'], how='left')
            df = df.merge(df_adj, on=['ä»£ç ', 'æ—¥æœŸ'], how='left')
            
            return df
        except Exception as e:
            print(f"âŒ åˆå¹¶ {trade_date} æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def fetch_adj_factor_by_date(self, trade_date: str) -> pd.DataFrame:
        """èŽ·å–å…¨å¸‚åœºå¤æƒå› å­"""
        try:
            df = self.pro.adj_factor(trade_date=trade_date)
            if df is not None and not df.empty:
                df = df.rename(columns={'ts_code': 'ä»£ç ', 'trade_date': 'æ—¥æœŸ', 'adj_factor': 'å¤æƒå› å­'})
                df['ä»£ç '] = df['ä»£ç '].str[:6]
                return df[['ä»£ç ', 'æ—¥æœŸ', 'å¤æƒå› å­']]
        except Exception as e:
            print(f"âš ï¸ èŽ·å– {trade_date} å¤æƒå› å­å¤±è´¥: {e}")
        return pd.DataFrame()
    
    # ==================== SQLite å­˜å‚¨ ====================
    
    def save_to_database(self, df: pd.DataFrame) -> int:
        """
        å°†æ•°æ®ä¿å­˜åˆ° SQLiteï¼ˆä½¿ç”¨ REPLACE å®žçŽ° upsertï¼‰
        
        Returns:
            æ’å…¥/æ›´æ–°çš„è®°å½•æ•°
        """
        if df.empty:
            return 0
        
        # ç¡®ä¿åˆ—é¡ºåºå’Œæ•°æ®åº“ä¸€è‡´
        columns = [
            'æ—¥æœŸ', 'ä»£ç ', 'åç§°', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½Ž', 'æ”¶ç›˜', 'æ˜¨æ”¶', 
            'æ¶¨è·Œé¢', 'æ¶¨è·Œå¹…', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æ¢æ‰‹çŽ‡', 'é‡æ¯”',
            'PE', 'PE_TTM', 'PB', 'æ€»å¸‚å€¼', 'æµé€šå¸‚å€¼', 'æ€»è‚¡æœ¬', 'æµé€šè‚¡æœ¬',
            'å¤æƒå› å­'
        ]
        
        # æ·»åŠ ç¼ºå¤±çš„åˆ—
        for col in columns:
            if col not in df.columns:
                df[col] = None
        
        df_to_save = df[columns]
        
        with sqlite3.connect(self.db_path) as conn:
            # ä½¿ç”¨ REPLACE INTO å®žçŽ° upsert
            placeholders = ', '.join(['?' for _ in columns])
            cols_str = ', '.join(columns)
            sql = f'REPLACE INTO daily_data ({cols_str}) VALUES ({placeholders})'
            
            # æ‰¹é‡æ’å…¥
            data = df_to_save.values.tolist()
            conn.executemany(sql, data)
            conn.commit()
            
            return len(data)
    
    def get_synced_dates(self) -> List[str]:
        """èŽ·å–å·²åŒæ­¥çš„æ—¥æœŸåˆ—è¡¨"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('SELECT DISTINCT æ—¥æœŸ FROM daily_data ORDER BY æ—¥æœŸ')
            return [row[0] for row in cursor.fetchall()]
    
    def get_last_synced_date(self) -> Optional[str]:
        """èŽ·å–æœ€åŽåŒæ­¥çš„æ—¥æœŸ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('SELECT MAX(æ—¥æœŸ) FROM daily_data')
            result = cursor.fetchone()
            return result[0] if result else None
    
    def get_stock_count(self) -> int:
        """èŽ·å–è‚¡ç¥¨æ•°é‡"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('SELECT COUNT(DISTINCT ä»£ç ) FROM daily_data')
            return cursor.fetchone()[0]
    
    def get_record_count(self) -> int:
        """èŽ·å–æ€»è®°å½•æ•°"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('SELECT COUNT(*) FROM daily_data')
            return cursor.fetchone()[0]
    
    # ==================== ä¸»åŒæ­¥é€»è¾‘ ====================
    
    def sync_all_stocks(
        self, 
        days: int = 120, 
        start_date: str = None,
        end_date: str = None,
        progress_callback=None,
        force: bool = False
    ) -> bool:
        """
        åŒæ­¥å…¨å¸‚åœºåŽ†å²æ•°æ®ï¼ˆSQLite å­˜å‚¨ï¼‰
        
        ç­–ç•¥ï¼š
        1. æŒ‰æ—¥æœŸèŽ·å–å…¨å¸‚åœºæ•°æ®
        2. åˆå¹¶ daily + daily_basic
        3. æ‰¹é‡å†™å…¥ SQLiteï¼ˆä½¿ç”¨äº‹åŠ¡ï¼‰
        """
        if not self._acquire_lock():
            print("âš ï¸ å¦ä¸€ä¸ªåŒæ­¥ä»»åŠ¡æ­£åœ¨è¿è¡Œ")
            if progress_callback:
                progress_callback(0, 0, "å·²æœ‰ä»»åŠ¡è¿è¡Œ", "")
            return False
        
        try:
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            today = datetime.now()
            if not end_date:
                end_date = today.strftime("%Y%m%d")
            if not start_date:
                start_date = (today - timedelta(days=days)).strftime("%Y%m%d")
            
            # æ ¼å¼åŒ–æ—¥æœŸï¼ˆå¦‚æžœæ˜¯ 2024-01-01 æ ¼å¼è½¬ä¸º 20240101ï¼‰
            start_date = start_date.replace("-", "").replace("/", "")
            end_date = end_date.replace("-", "").replace("/", "")

            print(f"ðŸš€ å¼€å§‹åŒæ­¥ {start_date} ~ {end_date} é—´æ•°æ®ï¼ˆSQLite å­˜å‚¨æ¨¡å¼ï¼‰...")
            
            # èŽ·å–äº¤æ˜“æ—¥åˆ—è¡¨
            trading_days = self.get_trading_days(start_date, end_date)
            if not trading_days:
                print("âŒ èŽ·å–äº¤æ˜“æ—¥åŽ†å¤±è´¥")
                return False
            
            print(f"ðŸ“… äº¤æ˜“æ—¥èŒƒå›´: {trading_days[0]} ~ {trading_days[-1]}ï¼Œå…± {len(trading_days)} ä¸ªäº¤æ˜“æ—¥")
            
            # å¢žé‡æ¨¡å¼ï¼šæŽ’é™¤å·²ç»åŒæ­¥è¿‡ä¸”æ•°æ®å®Œæ•´çš„æ—¥æœŸ
            if not force:
                # æ£€æŸ¥å·²åŒæ­¥çš„æ—¥æœŸä¸­ï¼Œå“ªäº›æ˜¯ç¼ºå¤±â€˜å¤æƒå› å­â€™çš„
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.execute("SELECT DISTINCT æ—¥æœŸ FROM daily_data WHERE å¤æƒå› å­ IS NOT NULL")
                    complete_dates = {row[0] for row in cursor.fetchall()}
                
                trading_days = [d for d in trading_days if d not in complete_dates]
                
                if not trading_days:
                    print("âœ… æ‰€é€‰èŒƒå›´å†…çš„äº¤æ˜“æ—¥å·²å…¨éƒ¨åŒæ­¥ä¸”æ•°æ®å®Œæ•´")
                    # å³ä½¿æ—¥æœŸåŒæ­¥å®Œäº†ï¼Œä¹Ÿè¦æ£€æŸ¥å¹¶è®¡ç®—æŒ‡æ ‡
                    self.recompute_technical_indicators()
                    return True
                
                print(f"ðŸ“Š å¢žé‡åŒæ­¥: éœ€è¦èŽ·å–/è¡¥å…¨ {len(trading_days)} ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®")
            else:
                print("âš ï¸ å¼ºåˆ¶åŒæ­¥æ¨¡å¼ï¼Œæ¸…ç©ºæ•°æ®åº“...")
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute('DELETE FROM daily_data')
                    conn.commit()
            
            # åŒæ­¥
            total_records = 0
            self._stop_requested = False
            
            for i, trade_date in enumerate(trading_days):
                if self._stop_requested:
                    print("â¹ï¸ æ”¶åˆ°åœæ­¢ä¿¡å·")
                    break
                
                # èŽ·å–å¹¶åˆå¹¶æ•°æ®
                df = self.fetch_and_merge_by_date(trade_date)
                
                if not df.empty:
                    # è‡ªåŠ¨æ›´æ–°åŸºç¡€ä¿¡æ¯ï¼šå¦‚æžœå‘çŽ°æ–°è‚¡ç¥¨ä»£ç 
                    self._check_for_new_stocks(df['ä»£ç '].unique())
                    
                    # ä¿å­˜åˆ° SQLite
                    count = self.save_to_database(df)
                    total_records += count
                    status = f"èŽ·å– {len(df)} æ¡ï¼Œç´¯è®¡ {total_records} æ¡"
                else:
                    status = "æ— æ•°æ®"
                
                if progress_callback:
                    progress_callback(i + 1, len(trading_days), trade_date, status)
                
                # çŸ­æš‚é—´éš”
                time.sleep(0.15)
            
            # é‡æ–°åŒæ­¥ä¸€æ¬¡åŸºç¡€ä¿¡æ¯ï¼Œç¡®ä¿åç§°æœ€æ–°
            self.sync_stock_basic()
            
            # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆå‰å¤æƒã€MAã€VMAï¼‰
            print("ðŸ“ˆ æ­£åœ¨è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆå‰å¤æƒã€å‡çº¿ç­‰ï¼‰...")
            if progress_callback:
                progress_callback(len(trading_days), len(trading_days), "è®¡ç®—ä¸­", "è®¡ç®—å‡çº¿æŒ‡æ ‡...")
            self.recompute_technical_indicators()
            
            # ä¿å­˜å…ƒæ•°æ®ï¼ˆåæ˜ æ•°æ®åº“å…¨é‡çŠ¶æ€ï¼‰
            all_synced_dates = self.get_synced_dates()
            metadata = {
                "last_sync_date": datetime.now().isoformat(),
                "total_stocks": self.get_stock_count(),
                "total_records": self.get_record_count(),
                "total_days": len(all_synced_dates),
                "date_range": {
                    "start": all_synced_dates[0] if all_synced_dates else None,
                    "end": all_synced_dates[-1] if all_synced_dates else None
                },
                "storage": "sqlite",
                "db_file": os.path.relpath(self.db_path)
            }
            self.save_metadata(metadata)
            
            print(f"âœ… åŒæ­¥å®Œæˆ: {len(trading_days)} ä¸ªäº¤æ˜“æ—¥, {self.get_stock_count()} åªè‚¡ç¥¨, {total_records} æ¡è®°å½•")
            return True
        
        finally:
            self._release_lock()
    
    def _check_for_new_stocks(self, current_codes: List[str]):
        """æ£€æŸ¥æ˜¯å¦æœ‰æ–°è‚¡ç¥¨å‡ºçŽ°ï¼Œå¦‚æžœæœ‰åˆ™åŒæ­¥åŸºç¡€è¡¨"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("SELECT ä»£ç  FROM stock_basic")
                synced_codes = {row[0] for row in cursor.fetchall()}
            
            new_codes = set(current_codes) - synced_codes
            if new_codes:
                print(f"ðŸ“¢ å‘çŽ° {len(new_codes)} åªæ–°è‚¡ç¥¨è®°å½•ï¼Œæ›´æ–°åŸºç¡€ä¿¡æ¯è¡¨...")
                self.sync_stock_basic()
        except:
            pass

    # ==================== æ•°æ®æŸ¥è¯¢ ====================
    
    def get_stock_history(self, code: str, days: int = None) -> pd.DataFrame:
        """èŽ·å–å•åªè‚¡ç¥¨åŽ†å²æ•°æ®"""
        sql = '''
            SELECT d.*, b.åç§° 
            FROM daily_data d
            LEFT JOIN stock_basic b ON d.ä»£ç  = b.ä»£ç 
            WHERE d.ä»£ç  = ? 
            ORDER BY d.æ—¥æœŸ
        '''
        params = [code]
        
        if days:
            sql = f'''
                SELECT d.*, b.åç§° 
                FROM daily_data d
                LEFT JOIN stock_basic b ON d.ä»£ç  = b.ä»£ç 
                WHERE d.ä»£ç  = ? 
                ORDER BY d.æ—¥æœŸ DESC 
                LIMIT ?
            '''
            params = [code, days]
        
        with sqlite3.connect(self.db_path) as conn:
            df = pd.read_sql_query(sql, conn, params=params)
        
        if days:
            df = df.sort_values('æ—¥æœŸ')
        
        return df

    def get_daily_data(self, trade_date: str) -> pd.DataFrame:
        """èŽ·å–æŸä¸€å¤©çš„å…¨å¸‚åœºæ•°æ®"""
        sql = '''
            SELECT d.*, b.åç§° 
            FROM daily_data d
            LEFT JOIN stock_basic b ON d.ä»£ç  = b.ä»£ç 
            WHERE d.æ—¥æœŸ = ?
        '''
        with sqlite3.connect(self.db_path) as conn:
            return pd.read_sql_query(sql, conn, params=[trade_date])
    
    def query(self, sql: str, params: tuple = None) -> pd.DataFrame:
        """æ‰§è¡Œè‡ªå®šä¹‰ SQL æŸ¥è¯¢"""
        with sqlite3.connect(self.db_path) as conn:
            return pd.read_sql_query(sql, conn, params=params)
    
    def get_stocks_by_filter(
        self,
        trade_date: str = None,
        min_pct_chg: float = None,
        max_pct_chg: float = None,
        min_pe: float = None,
        max_pe: float = None,
        min_pb: float = None,
        max_pb: float = None,
        max_market_cap: float = None,  # äº¿
        min_turnover: float = None,
        limit: int = 100
    ) -> pd.DataFrame:
        """
        æŒ‰æ¡ä»¶ç­›é€‰è‚¡ç¥¨
        
        ç¤ºä¾‹ï¼š
            # èŽ·å–æ¶¨åœè‚¡
            get_stocks_by_filter(trade_date='20260130', min_pct_chg=9.5)
            
            # èŽ·å–ä½Ž PE å°ç›˜è‚¡
            get_stocks_by_filter(max_pe=20, max_market_cap=50)
        """
        conditions = []
        params = []
        
        if trade_date:
            conditions.append('æ—¥æœŸ = ?')
            params.append(trade_date)
        
        if min_pct_chg is not None:
            conditions.append('æ¶¨è·Œå¹… >= ?')
            params.append(min_pct_chg)
        
        if max_pct_chg is not None:
            conditions.append('æ¶¨è·Œå¹… <= ?')
            params.append(max_pct_chg)
        
        if min_pe is not None:
            conditions.append('PE >= ?')
            params.append(min_pe)
        
        if max_pe is not None:
            conditions.append('PE <= ?')
            params.append(max_pe)
        
        if min_pb is not None:
            conditions.append('PB >= ?')
            params.append(min_pb)
        
        if max_pb is not None:
            conditions.append('PB <= ?')
            params.append(max_pb)
        
        if max_market_cap is not None:
            conditions.append('æµé€šå¸‚å€¼ <= ?')
            params.append(max_market_cap * 1e4)  # äº¿ -> ä¸‡
        
        if min_turnover is not None:
            conditions.append('æ¢æ‰‹çŽ‡ >= ?')
            params.append(min_turnover)
        
        where_clause = ' AND '.join(conditions) if conditions else '1=1'
        sql = f'''
            SELECT * FROM daily_data 
            WHERE {where_clause}
            ORDER BY æ—¥æœŸ DESC, æ¶¨è·Œå¹… DESC
            LIMIT ?
        '''
        params.append(limit)
        
        with sqlite3.connect(self.db_path) as conn:
            return pd.read_sql_query(sql, conn, params=params)
    
    def get_zt_stocks(self, trade_date: str) -> pd.DataFrame:
        """èŽ·å–æ¶¨åœè‚¡ï¼ˆæ¶¨å¹… >= 9.5%ï¼‰"""
        return self.get_stocks_by_filter(trade_date=trade_date, min_pct_chg=9.5, limit=500)
    
    def get_sync_status_info(self) -> dict:
        """èŽ·å–åŒæ­¥çŠ¶æ€ä¿¡æ¯"""
        metadata = self.get_metadata()
        
        db_size = 0
        if os.path.exists(self.db_path):
            db_size = os.path.getsize(self.db_path) / 1024 / 1024
        
        return {
            "last_sync": metadata.get("last_sync_date"),
            "total_stocks": self.get_stock_count(),
            "total_records": self.get_record_count(),
            "days": metadata.get("days", 0),
            "date_range": metadata.get("date_range", {}),
            "db_size_mb": round(db_size, 2),
            "storage": "sqlite",
            "db_file": self.db_path
        }


# å‘½ä»¤è¡Œå…¥å£
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Aè‚¡æ•°æ®åŒæ­¥å·¥å…·ï¼ˆSQLite å­˜å‚¨ï¼‰")
    parser.add_argument("--days", type=int, default=120, help="åŒæ­¥å¤©æ•°")
    parser.add_argument("--status", action="store_true", help="æŸ¥çœ‹çŠ¶æ€")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶å…¨é‡åŒæ­¥")
    parser.add_argument("--query", type=str, help="æ‰§è¡Œ SQL æŸ¥è¯¢")
    
    args = parser.parse_args()
    sync = DataSyncService()
    
    if args.status:
        status = sync.get_sync_status_info()
        print("ðŸ“Š åŒæ­¥çŠ¶æ€:")
        print(f"  æ•°æ®åº“: {status['db_file']}")
        print(f"  å¤§å°: {status['db_size_mb']} MB")
        print(f"  è‚¡ç¥¨æ•°é‡: {status['total_stocks']}")
        print(f"  è®°å½•æ€»æ•°: {status['total_records']}")
        print(f"  æ—¥æœŸèŒƒå›´: {status['date_range']}")
        print(f"  æœ€åŽåŒæ­¥: {status['last_sync']}")
    elif args.query:
        print(f"æ‰§è¡ŒæŸ¥è¯¢: {args.query}")
        df = sync.query(args.query)
        print(df.to_string())
    else:
        def progress(current, total, date, status):
            pct = current / total * 100 if total > 0 else 0
            print(f"[{current}/{total}] {pct:.0f}% | {date} | {status}")
        
        sync.sync_all_stocks(days=args.days, progress_callback=progress, force=args.force)
