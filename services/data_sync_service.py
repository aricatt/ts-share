"""
æœ¬åœ°æ•°æ®åŒæ­¥æœåŠ¡
è´Ÿè´£æ‹‰å–å’Œç®¡ç†Aè‚¡å†å²æ•°æ®ï¼ˆæŒ‰è‚¡ç¥¨ä»£ç åˆ†åŒºå­˜å‚¨ï¼‰
"""
import os
import json
import akshare as ak
import pandas as pd
from datetime import datetime, timedelta
from typing import Optional, List
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import random

# å¢åŠ å…¨å±€ User-Agent ä¼ªè£…
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def _patched_akshare_requests():
    """
    è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§æŠ€å·§ï¼šå°è¯•å½±å“å…¨å±€ requests è¡Œä¸ºï¼Œ
    è™½ç„¶ akshare å†…éƒ¨è‡ªå»º sessionï¼Œä½†æˆ‘ä»¬å¯ä»¥å°è¯•æä¾›ä¸€ä¸ªç¨³å¥çš„ UA åˆ—è¡¨
    """
    USER_AGENTS = [
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ]
    return random.choice(USER_AGENTS)


class DataSyncService:
    """
    æœ¬åœ°æ•°æ®åŒæ­¥æœåŠ¡
    
    å­˜å‚¨ç»“æ„ï¼ˆæŒ‰è‚¡ç¥¨ä»£ç åˆ†åŒºï¼‰ï¼š
        data/
        â”œâ”€â”€ stocks/              # æŒ‰è‚¡ç¥¨ä»£ç åˆ†åŒº
        â”‚   â”œâ”€â”€ 000001.parquet   # å¹³å®‰é“¶è¡Œ120å¤©å†å²
        â”‚   â”œâ”€â”€ 000002.parquet   # ä¸‡ç§‘A
        â”‚   â””â”€â”€ ...
        â””â”€â”€ metadata.json        # å…ƒæ•°æ®
    
    ä½¿ç”¨åœºæ™¯ï¼š
        - AkShare è·å–æ¶¨åœè‚¡æ± ï¼ˆå®æ—¶ï¼‰
        - ä»æœ¬åœ°è·å–å•åªè‚¡ç¥¨å†å²æ•°æ®ï¼ˆå¿«é€Ÿï¼‰
    """
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.stocks_dir = os.path.join(data_dir, "stocks")
        self.metadata_path = os.path.join(data_dir, "metadata.json")
        self._stop_requested = False # åœæ­¢æ ‡å¿—
        
        # åˆ›å»ºç›®å½•
        os.makedirs(self.stocks_dir, exist_ok=True)

    def request_stop(self):
        """è¯·æ±‚åœæ­¢åŒæ­¥"""
        self._stop_requested = True

    def get_metadata(self) -> dict:
        """è·å–å…ƒæ•°æ®"""
        if os.path.exists(self.metadata_path):
            with open(self.metadata_path, 'r') as f:
                return json.load(f)
        return {
            "last_sync_date": None,
            "total_stocks": 0,
            "days": 0,
            "date_range": {},
        }
    
    def save_metadata(self, metadata: dict):
        """ä¿å­˜å…ƒæ•°æ®"""
        with open(self.metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    def get_all_stock_codes(self) -> List[str]:
        """è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨ä»£ç """
        try:
            df = ak.stock_info_a_code_name()
            return df['code'].tolist()
        except Exception as e:
            print(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def get_synced_stocks(self) -> set:
        """è·å–å·²åŒæ­¥çš„è‚¡ç¥¨ä»£ç """
        synced = set()
        if os.path.exists(self.stocks_dir):
            for f in os.listdir(self.stocks_dir):
                if f.endswith('.parquet'):
                    code = f.replace('.parquet', '')
                    synced.add(code)
        return synced
    
    def sync_single_stock(self, code: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """
        åŒæ­¥å•åªè‚¡ç¥¨çš„å†å²æ•°æ®
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            start_date: èµ·å§‹æ—¥æœŸ YYYYMMDD
            end_date: ç»“æŸæ—¥æœŸ YYYYMMDD
        
        Returns:
            Kçº¿æ•°æ® DataFrame
        """
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                df = ak.stock_zh_a_hist(
                    symbol=code,
                    period="daily",
                    start_date=start_date,
                    end_date=end_date,
                    adjust="qfq"
                )
                
                if df is not None and not df.empty:
                    df['ä»£ç '] = code
                
                return df
            except Exception as e:
                if attempt < max_retries - 1:
                    # é‡è¯•å‰ç­‰å¾…ï¼Œé€æ¬¡å¢åŠ ç­‰å¾…æ—¶é—´
                    time.sleep(1 + attempt * 2)
                else:
                    return None
        
        return None
    
    def check_api_health(self) -> bool:
        """æ£€æµ‹ API æ˜¯å¦é€šç•…ï¼ˆå¿ƒè·³æ£€æµ‹ï¼‰"""
        try:
            # éšæœºé€‰ä¸€åªæƒé‡è‚¡æµ‹è¯•ï¼Œå¦‚ 000001
            df = ak.stock_zh_a_hist(
                symbol="000001",
                period="daily",
                start_date=(datetime.now() - timedelta(days=10)).strftime("%Y%m%d"),
                end_date=(datetime.now() - timedelta(days=1)).strftime("%Y%m%d"),
                adjust="qfq"
            )
            return df is not None and not df.empty
        except Exception:
            return False

    def _sync_stock_incremental(self, code: str, target_days: int, end_date: str) -> dict:
        """
        å¢é‡åŒæ­¥å•åªè‚¡ç¥¨
        
        å¢é‡é€»è¾‘ï¼š
        - æ£€æŸ¥å¤´éƒ¨ï¼šæ˜¯å¦éœ€è¦è¡¥å……æ›´æ—©çš„å†å²æ•°æ®
        - æ£€æŸ¥å°¾éƒ¨ï¼šæ˜¯å¦éœ€è¦è¿½åŠ æ–°æ•°æ®
        """
        file_path = os.path.join(self.stocks_dir, f"{code}.parquet")
        target_start = datetime.strptime(end_date, "%Y%m%d") - timedelta(days=target_days)
        target_start_str = target_start.strftime("%Y%m%d")
        
        try:
            if os.path.exists(file_path):
                existing_df = pd.read_parquet(file_path)
                if existing_df.empty:
                    df = self.sync_single_stock(code, target_start_str, end_date)
                    if df is not None and not df.empty:
                        df.to_parquet(file_path, index=False)
                        return {"status": "new", "new_records": len(df)}
                    return {"status": "failed", "new_records": 0}
                
                existing_df['æ—¥æœŸ'] = pd.to_datetime(existing_df['æ—¥æœŸ'])
                first_date = existing_df['æ—¥æœŸ'].min()
                last_date = existing_df['æ—¥æœŸ'].max()
                
                new_records = 0
                dfs_to_merge = [existing_df]
                
                # 1. æ£€æŸ¥å¤´éƒ¨
                if first_date > target_start:
                    head_end = (first_date - timedelta(days=1)).strftime("%Y%m%d")
                    head_df = self.sync_single_stock(code, target_start_str, head_end)
                    if head_df is not None and not head_df.empty:
                        dfs_to_merge.insert(0, head_df)
                        new_records += len(head_df)
                
                # 2. æ£€æŸ¥å°¾éƒ¨
                if last_date.strftime("%Y%m%d") < end_date:
                    tail_start = (last_date + timedelta(days=1)).strftime("%Y%m%d")
                    tail_df = self.sync_single_stock(code, tail_start, end_date)
                    if tail_df is not None and not tail_df.empty:
                        dfs_to_merge.append(tail_df)
                        new_records += len(tail_df)
                
                if new_records > 0:
                    combined = pd.concat(dfs_to_merge, ignore_index=True)
                    combined['æ—¥æœŸ'] = pd.to_datetime(combined['æ—¥æœŸ'])
                    combined = combined.drop_duplicates(subset=['æ—¥æœŸ'], keep='last')
                    combined = combined.sort_values('æ—¥æœŸ')
                    combined.to_parquet(file_path, index=False)
                    return {"status": "updated", "new_records": new_records}
                else:
                    return {"status": "skipped", "new_records": 0}
            else:
                df = self.sync_single_stock(code, target_start_str, end_date)
                if df is not None and not df.empty:
                    df.to_parquet(file_path, index=False)
                    return {"status": "new", "new_records": len(df)}
                return {"status": "failed", "new_records": 0}
        except Exception:
            return {"status": "failed", "new_records": 0}
    
    def sync_all_stocks(
        self, 
        days: int = 120, 
        max_workers: int = 1, # å»ºè®®é»˜è®¤å•çº¿ç¨‹ï¼Œé¿å…å°ç¦
        progress_callback=None,
        force: bool = False
    ) -> bool:
        """
        åŒæ­¥æ‰€æœ‰è‚¡ç¥¨çš„å†å²æ•°æ®
        
        ç­–ç•¥ï¼š
        - é»˜è®¤å•çº¿ç¨‹ï¼Œé¿å…é«˜é¢‘è¯·æ±‚è§¦å‘ IP å°é”
        - å®šæœŸè¿›è¡Œ API å¥åº·æ£€æŸ¥
        - è§¦å‘é™æµåè‡ªåŠ¨è¿›å…¥å†·å´ï¼Œæ”¯æŒé‡è¯•
        """
        print(f"å¼€å§‹åŒæ­¥æ‰€æœ‰è‚¡ç¥¨çš„ {days} å¤©å†å²æ•°æ®...")
        
        all_codes = self.get_all_stock_codes()
        if not all_codes:
            return False
        
        # è®¡ç®—ç»“æŸæ—¥æœŸ
        today = datetime.now()
        yesterday = today - timedelta(days=1)
        end_date = yesterday.strftime("%Y%m%d")
        start_date = (yesterday - timedelta(days=days)).strftime("%Y%m%d")
        
        if force:
            for f in os.listdir(self.stocks_dir):
                if f.endswith('.parquet'):
                    os.remove(os.path.join(self.stocks_dir, f))
        
        stats = {"new": 0, "updated": 0, "skipped": 0, "failed": 0, "new_records": 0}
        cool_down_minutes = 5
        consecutive_fails = 0
        
        # ä½¿ç”¨å•çº¿ç¨‹æˆ–å¤šçº¿ç¨‹æ‰§è¡Œ
        self._stop_requested = False
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {}
            for code in all_codes:
                futures[executor.submit(self._sync_stock_incremental, code, days, end_date)] = code
            
            for i, future in enumerate(as_completed(futures)):
                # æ£€æŸ¥ä¸­æ–­æŒ‡ä»¤
                if self._stop_requested:
                    print("æ¥æ”¶åˆ°æ‰‹åŠ¨ä¸­æ–­æŒ‡ä»¤ï¼Œæ­£åœ¨åœæ­¢åŒæ­¥...")
                    if progress_callback:
                        progress_callback(i, len(all_codes), "N/A", "æ‰‹åŠ¨ä¸­æ–­ä¸­...")
                    break
                
                code = futures[future]
                
                # æ¯50åªè‚¡ç¥¨åšä¸€æ¬¡å¥åº·æ£€æŸ¥
                if (i + 1) % 50 == 0:
                    if not self.check_api_health():
                        print(f"æ£€æµ‹åˆ° API å—é™ï¼Œè¿›å…¥å†·å´ {cool_down_minutes} åˆ†é’Ÿ...")
                        if progress_callback:
                            progress_callback(i + 1, len(all_codes), code, f"è§¦å‘é™æµï¼Œè¿›å…¥å†·å´({cool_down_minutes}min)")
                        time.sleep(cool_down_minutes * 60)
                        
                        # å†·å´åå†æ¬¡æ£€æŸ¥
                        while not self.check_api_health():
                            print("ä¾ç„¶å—é™ï¼Œç»§ç»­ç­‰å¾…...")
                            time.sleep(60)
                
                try:
                    result = future.result(timeout=60) # å¢åŠ è¶…æ—¶æ§åˆ¶
                    status = result["status"]
                    stats[status] += 1
                    stats["new_records"] += result["new_records"]
                    
                    if status == "failed":
                        consecutive_fails += 1
                    else:
                        consecutive_fails = 0
                except Exception:
                    stats["failed"] += 1
                    consecutive_fails += 1
                
                # å¦‚æœè¿ç»­å¤±è´¥å¤ªå¤šï¼Œå¾ˆå¯èƒ½å·²ç»ç”± IP è¢«å°ï¼Œåœæ­¢
                if consecutive_fails > 10:
                    print("è¿ç»­å¤±è´¥è¶…è¿‡10æ¬¡ï¼Œå¼ºåˆ¶åœæ­¢åŒæ­¥ä»¥ä¿æŠ¤ IP")
                    if progress_callback:
                        progress_callback(i + 1, len(all_codes), code, "ä¸¥é‡é™æµï¼Œå·²åœæ­¢åŒæ­¥")
                    return False
                
                # è¿›åº¦å›è°ƒ
                if progress_callback:
                    status_text = "åŒæ­¥ä¸­" if stats["failed"] == 0 else f"æ­£åœ¨åŒæ­¥ (å¤±è´¥:{stats['failed']})"
                    progress_callback(i + 1, len(all_codes), code, status_text)
                
                # å¾®å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚é¢‘ç‡è¿‡é«˜
                if max_workers == 1:
                    time.sleep(0.5)
                elif (i + 1) % 10 == 0:
                    time.sleep(1)
        
        print(f"åŒæ­¥å®Œæˆ: æ–°å¢ {stats['new']}, æ›´æ–° {stats['updated']}, è·³è¿‡ {stats['skipped']}, å¤±è´¥ {stats['failed']}")
        
        metadata = {
            "last_sync_date": datetime.now().isoformat(),
            "total_stocks": len(self.get_synced_stocks()),
            "days": days,
            "date_range": {"start": start_date, "end": end_date}
        }
        self.save_metadata(metadata)
        return True
    
    def get_stock_history(self, code: str) -> pd.DataFrame:
        """
        ä»æœ¬åœ°è·å–å•åªè‚¡ç¥¨çš„å†å²æ•°æ®
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
        
        Returns:
            è‚¡ç¥¨å†å²æ•°æ® DataFrame
        """
        file_path = os.path.join(self.stocks_dir, f"{code}.parquet")
        
        if os.path.exists(file_path):
            return pd.read_parquet(file_path)
        
        return pd.DataFrame()
    
    def get_stock_history_or_fetch(self, code: str, days: int = 120) -> pd.DataFrame:
        """
        è·å–è‚¡ç¥¨å†å²æ•°æ®ï¼Œæœ¬åœ°æ²¡æœ‰åˆ™ä» AkShare è·å–
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            days: å¤©æ•°
        
        Returns:
            è‚¡ç¥¨å†å²æ•°æ® DataFrame
        """
        # å…ˆå°è¯•ä»æœ¬åœ°è·å–
        df = self.get_stock_history(code)
        if not df.empty:
            return df
        
        # æœ¬åœ°æ²¡æœ‰ï¼Œä» AkShare è·å–
        print(f"æœ¬åœ°æ—  {code} æ•°æ®ï¼Œä» AkShare è·å–...")
        yesterday = datetime.now() - timedelta(days=1)
        end_date = yesterday.strftime("%Y%m%d")
        start_date = (yesterday - timedelta(days=days)).strftime("%Y%m%d")
        
        df = self.sync_single_stock(code, start_date, end_date)
        
        # ä¿å­˜åˆ°æœ¬åœ°
        if df is not None and not df.empty:
            file_path = os.path.join(self.stocks_dir, f"{code}.parquet")
            df.to_parquet(file_path, index=False)
        
        return df if df is not None else pd.DataFrame()
    
    def update_stock(self, code: str) -> bool:
        """
        æ›´æ–°å•åªè‚¡ç¥¨çš„æ•°æ®ï¼ˆå¢é‡æ›´æ–°ï¼‰
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        existing_df = self.get_stock_history(code)
        
        if existing_df.empty:
            # æ²¡æœ‰å†å²æ•°æ®ï¼Œå…¨é‡è·å–
            df = self.get_stock_history_or_fetch(code)
            return not df.empty
        
        # è·å–å·²æœ‰æ•°æ®çš„æœ€åæ—¥æœŸ
        existing_df['æ—¥æœŸ'] = pd.to_datetime(existing_df['æ—¥æœŸ'])
        last_date = existing_df['æ—¥æœŸ'].max()
        
        # ä»æœ€åæ—¥æœŸ+1å¤©å¼€å§‹è·å–
        start_date = (last_date + timedelta(days=1)).strftime("%Y%m%d")
        yesterday = datetime.now() - timedelta(days=1)
        end_date = yesterday.strftime("%Y%m%d")
        
        if start_date > end_date:
            print(f"{code} å·²æ˜¯æœ€æ–°æ•°æ®")
            return True
        
        # è·å–å¢é‡æ•°æ®
        new_df = self.sync_single_stock(code, start_date, end_date)
        
        if new_df is not None and not new_df.empty:
            # åˆå¹¶æ•°æ®
            combined = pd.concat([existing_df, new_df], ignore_index=True)
            combined = combined.drop_duplicates(subset=['æ—¥æœŸ'], keep='last')
            combined = combined.sort_values('æ—¥æœŸ')
            
            # ä¿å­˜
            file_path = os.path.join(self.stocks_dir, f"{code}.parquet")
            combined.to_parquet(file_path, index=False)
            print(f"æ›´æ–° {code}: æ–°å¢ {len(new_df)} æ¡è®°å½•")
        
        return True
    
    def get_sync_status(self) -> dict:
        """è·å–åŒæ­¥çŠ¶æ€"""
        metadata = self.get_metadata()
        
        # ç»Ÿè®¡æœ¬åœ°æ–‡ä»¶
        synced_stocks = self.get_synced_stocks()
        total_size = 0
        if os.path.exists(self.stocks_dir):
            for f in os.listdir(self.stocks_dir):
                if f.endswith('.parquet'):
                    total_size += os.path.getsize(os.path.join(self.stocks_dir, f))
        
        return {
            "last_sync": metadata.get("last_sync_date"),
            "total_stocks": len(synced_stocks),
            "days": metadata.get("days", 0),
            "date_range": metadata.get("date_range", {}),
            "total_size_mb": round(total_size / 1024 / 1024, 2),
        }


# å‘½ä»¤è¡Œå…¥å£
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Aè‚¡æ•°æ®åŒæ­¥å·¥å…·")
    parser.add_argument("--days", type=int, default=120, help="åŒæ­¥å¤©æ•°")
    parser.add_argument("--workers", type=int, default=3, help="å¹¶å‘æ•°")
    parser.add_argument("--status", action="store_true", help="æŸ¥çœ‹åŒæ­¥çŠ¶æ€")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶å…¨é‡åŒæ­¥")
    
    args = parser.parse_args()
    
    sync = DataSyncService()
    
    if args.status:
        status = sync.get_sync_status()
        print("ğŸ“Š åŒæ­¥çŠ¶æ€:")
        print(f"  æœ€ååŒæ­¥: {status['last_sync']}")
        print(f"  è‚¡ç¥¨æ•°é‡: {status['total_stocks']}")
        print(f"  åŒæ­¥å¤©æ•°: {status['days']}")
        print(f"  æ•°æ®å¤§å°: {status['total_size_mb']} MB")
        print(f"  æ—¥æœŸèŒƒå›´: {status['date_range']}")
    else:
        if args.force:
            print("âš ï¸ å¼ºåˆ¶å…¨é‡åŒæ­¥æ¨¡å¼")
        else:
            print("ğŸ“Š å¢é‡åŒæ­¥æ¨¡å¼ï¼ˆå°†è·³è¿‡å·²åŒæ­¥çš„è‚¡ç¥¨ï¼‰")
        
        def progress(current, total, code):
            if current % 100 == 0:
                print(f"è¿›åº¦: {current}/{total} ({current/total*100:.1f}%)")
        
        sync.sync_all_stocks(
            days=args.days, 
            max_workers=args.workers, 
            progress_callback=progress,
            force=args.force
        )
