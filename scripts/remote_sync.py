#!/usr/bin/env python3
"""
è¿œç¨‹æ•°æ®åŒæ­¥è„šæœ¬ - ç‹¬ç«‹ç‰ˆ
==========================

åŠŸèƒ½ï¼š
- ä½é¢‘åŒæ­¥å…¨Aè‚¡å†å²Kçº¿æ•°æ®ï¼ˆé¿å…é™æµï¼‰
- æ–­ç‚¹ç»­ä¼ ï¼ˆä¸­æ–­åå¯ç»§ç»­ï¼‰
- å¢é‡æ›´æ–°ï¼ˆåªä¸‹è½½æ–°æ•°æ®ï¼‰
- æ‰“åŒ…å¯¼å‡ºï¼ˆæ–¹ä¾¿ä¸‹è½½åˆ°æœ¬åœ°ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
---------
# 1. å®‰è£…ä¾èµ–
pip install akshare pandas pyarrow tqdm

# 2. é¦–æ¬¡å…¨é‡åŒæ­¥ï¼ˆä½é¢‘æ¨¡å¼ï¼‰
python remote_sync.py --mode full --days 120 --delay 3

# 3. å¢é‡æ›´æ–°ï¼ˆæ—¥å¸¸ä½¿ç”¨ï¼‰
python remote_sync.py --mode incremental

# 4. æ‰“åŒ…æ•°æ®ï¼ˆä¸‹è½½åˆ°æœ¬åœ°ï¼‰
python remote_sync.py --mode export

# 5. æŸ¥çœ‹çŠ¶æ€
python remote_sync.py --mode status

ä½œè€…: TS-Share
"""

import os
import sys
import json
import time
import random
import argparse
import subprocess
from datetime import datetime, timedelta
from typing import Optional, Set, List

import akshare as ak
import pandas as pd
from tqdm import tqdm


# ============ é…ç½® ============
class Config:
    # æ•°æ®ç›®å½•
    DATA_DIR = "data"
    STOCKS_DIR = os.path.join(DATA_DIR, "stocks")
    METADATA_FILE = os.path.join(DATA_DIR, "metadata.json")
    PROGRESS_FILE = os.path.join(DATA_DIR, "sync_progress.json")
    
    # é»˜è®¤åŒæ­¥å¤©æ•°
    DEFAULT_DAYS = 120
    
    # è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰- ä½é¢‘æ¨¡å¼å…³é”®å‚æ•°
    DEFAULT_DELAY = 3.0      # æ¯æ¬¡è¯·æ±‚é—´éš”
    DELAY_JITTER = 1.0       # éšæœºæŠ–åŠ¨èŒƒå›´
    
    # é™æµä¿æŠ¤
    MAX_CONSECUTIVE_FAILS = 5    # è¿ç»­å¤±è´¥æ¬¡æ•°ä¸Šé™
    COOLDOWN_MINUTES = 10        # è§¦å‘é™æµåå†·å´æ—¶é—´
    HEALTH_CHECK_INTERVAL = 100  # æ¯Nåªè‚¡ç¥¨æ£€æŸ¥ä¸€æ¬¡APIå¥åº·
    
    # å¯¼å‡ºé…ç½®
    EXPORT_DIR = "export"


# ============ å·¥å…·å‡½æ•° ============
def log(msg: str, level: str = "INFO"):
    """æ‰“å°æ—¥å¿—"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [{level}] {msg}")


def random_delay(base: float, jitter: float = 1.0):
    """éšæœºå»¶è¿Ÿ"""
    delay = base + random.uniform(0, jitter)
    time.sleep(delay)


def ensure_dirs():
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    os.makedirs(Config.STOCKS_DIR, exist_ok=True)
    os.makedirs(Config.EXPORT_DIR, exist_ok=True)


# ============ å…ƒæ•°æ®ç®¡ç† ============
def load_metadata() -> dict:
    """åŠ è½½å…ƒæ•°æ®"""
    if os.path.exists(Config.METADATA_FILE):
        with open(Config.METADATA_FILE, 'r') as f:
            return json.load(f)
    return {
        "last_sync_date": None,
        "total_stocks": 0,
        "days": 0,
        "date_range": {},
    }


def save_metadata(metadata: dict):
    """ä¿å­˜å…ƒæ•°æ®"""
    with open(Config.METADATA_FILE, 'w') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)


def load_progress() -> dict:
    """åŠ è½½åŒæ­¥è¿›åº¦ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰"""
    if os.path.exists(Config.PROGRESS_FILE):
        with open(Config.PROGRESS_FILE, 'r') as f:
            return json.load(f)
    return {
        "completed_codes": [],
        "failed_codes": [],
        "last_code": None,
        "start_time": None,
    }


def save_progress(progress: dict):
    """ä¿å­˜åŒæ­¥è¿›åº¦"""
    with open(Config.PROGRESS_FILE, 'w') as f:
        json.dump(progress, f, indent=2, ensure_ascii=False)


def clear_progress():
    """æ¸…é™¤è¿›åº¦æ–‡ä»¶"""
    if os.path.exists(Config.PROGRESS_FILE):
        os.remove(Config.PROGRESS_FILE)


# ============ API å‡½æ•° ============
def get_all_stock_codes() -> List[str]:
    """è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨ä»£ç """
    log("æ­£åœ¨è·å–è‚¡ç¥¨åˆ—è¡¨...")
    try:
        df = ak.stock_info_a_code_name()
        codes = df['code'].tolist()
        log(f"è·å–åˆ° {len(codes)} åªè‚¡ç¥¨")
        return codes
    except Exception as e:
        log(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}", "ERROR")
        return []


def check_api_health() -> bool:
    """æ£€æŸ¥ API æ˜¯å¦å¥åº·"""
    try:
        end_date = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")
        start_date = (datetime.now() - timedelta(days=5)).strftime("%Y%m%d")
        df = ak.stock_zh_a_hist(
            symbol="000001",
            period="daily",
            start_date=start_date,
            end_date=end_date,
            adjust="qfq"
        )
        return df is not None and not df.empty
    except Exception:
        return False


def fetch_stock_history(code: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
    """è·å–å•åªè‚¡ç¥¨çš„å†å²æ•°æ®"""
    try:
        df = ak.stock_zh_a_hist(
            symbol=code,
            period="daily",
            start_date=start_date,
            end_date=end_date,
            adjust="qfq"
        )
        if df is not None and not df.empty:
            df['ä»£ç '] = code
        return df
    except Exception as e:
        return None


# ============ åŒæ­¥é€»è¾‘ ============
def get_synced_stocks() -> Set[str]:
    """è·å–å·²åŒæ­¥çš„è‚¡ç¥¨ä»£ç """
    synced = set()
    if os.path.exists(Config.STOCKS_DIR):
        for f in os.listdir(Config.STOCKS_DIR):
            if f.endswith('.parquet'):
                code = f.replace('.parquet', '')
                synced.add(code)
    return synced


def sync_single_stock(code: str, start_date: str, end_date: str) -> dict:
    """
    åŒæ­¥å•åªè‚¡ç¥¨ï¼ˆæ”¯æŒå¢é‡ï¼‰
    
    è¿”å›:
        {"status": "new"|"updated"|"skipped"|"failed", "records": æ•°é‡}
    """
    file_path = os.path.join(Config.STOCKS_DIR, f"{code}.parquet")
    
    try:
        if os.path.exists(file_path):
            # å·²æœ‰æ•°æ®ï¼Œå¢é‡æ›´æ–°
            existing_df = pd.read_parquet(file_path)
            if existing_df.empty:
                # ç©ºæ–‡ä»¶ï¼Œå…¨é‡è·å–
                df = fetch_stock_history(code, start_date, end_date)
                if df is not None and not df.empty:
                    df.to_parquet(file_path, index=False)
                    return {"status": "new", "records": len(df)}
                return {"status": "failed", "records": 0}
            
            existing_df['æ—¥æœŸ'] = pd.to_datetime(existing_df['æ—¥æœŸ'])
            last_date = existing_df['æ—¥æœŸ'].max()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            last_date_str = last_date.strftime("%Y%m%d")
            if last_date_str >= end_date:
                return {"status": "skipped", "records": 0}
            
            # è·å–å¢é‡æ•°æ®
            incr_start = (last_date + timedelta(days=1)).strftime("%Y%m%d")
            new_df = fetch_stock_history(code, incr_start, end_date)
            
            if new_df is not None and not new_df.empty:
                # åˆå¹¶æ•°æ®
                combined = pd.concat([existing_df, new_df], ignore_index=True)
                combined['æ—¥æœŸ'] = pd.to_datetime(combined['æ—¥æœŸ'])
                combined = combined.drop_duplicates(subset=['æ—¥æœŸ'], keep='last')
                combined = combined.sort_values('æ—¥æœŸ')
                combined.to_parquet(file_path, index=False)
                return {"status": "updated", "records": len(new_df)}
            
            return {"status": "skipped", "records": 0}
        else:
            # æ–°è‚¡ç¥¨ï¼Œå…¨é‡è·å–
            df = fetch_stock_history(code, start_date, end_date)
            if df is not None and not df.empty:
                df.to_parquet(file_path, index=False)
                return {"status": "new", "records": len(df)}
            return {"status": "failed", "records": 0}
    
    except Exception as e:
        return {"status": "failed", "records": 0}


def run_full_sync(days: int, delay: float, resume: bool = True):
    """
    å…¨é‡åŒæ­¥
    
    Args:
        days: åŒæ­¥å¤©æ•°
        delay: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
        resume: æ˜¯å¦æ–­ç‚¹ç»­ä¼ 
    """
    ensure_dirs()
    
    log(f"ğŸš€ å¼€å§‹å…¨é‡åŒæ­¥ (å¤©æ•°={days}, å»¶è¿Ÿ={delay}ç§’)")
    
    # è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
    all_codes = get_all_stock_codes()
    if not all_codes:
        log("æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œé€€å‡º", "ERROR")
        return
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    yesterday = datetime.now() - timedelta(days=1)
    end_date = yesterday.strftime("%Y%m%d")
    start_date = (yesterday - timedelta(days=days)).strftime("%Y%m%d")
    
    log(f"ğŸ“… æ•°æ®èŒƒå›´: {start_date} ~ {end_date}")
    
    # åŠ è½½è¿›åº¦ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    progress = load_progress() if resume else {"completed_codes": [], "failed_codes": []}
    completed_set = set(progress.get("completed_codes", []))
    
    if resume and completed_set:
        log(f"ğŸ“Œ æ–­ç‚¹ç»­ä¼ : å·²å®Œæˆ {len(completed_set)} åªè‚¡ç¥¨")
    
    # è¿‡æ»¤å¾…åŒæ­¥çš„è‚¡ç¥¨
    pending_codes = [c for c in all_codes if c not in completed_set]
    log(f"ğŸ“Š å¾…åŒæ­¥: {len(pending_codes)} åªè‚¡ç¥¨")
    
    # ç»Ÿè®¡
    stats = {"new": 0, "updated": 0, "skipped": 0, "failed": 0, "records": 0}
    consecutive_fails = 0
    
    # è®°å½•å¼€å§‹æ—¶é—´
    if not progress.get("start_time"):
        progress["start_time"] = datetime.now().isoformat()
    
    # å¼€å§‹åŒæ­¥
    try:
        for i, code in enumerate(tqdm(pending_codes, desc="åŒæ­¥è¿›åº¦")):
            # API å¥åº·æ£€æŸ¥
            if (i + 1) % Config.HEALTH_CHECK_INTERVAL == 0:
                log(f"ğŸ” è¿›è¡Œ API å¥åº·æ£€æŸ¥...")
                if not check_api_health():
                    log(f"âš ï¸ API å—é™ï¼Œè¿›å…¥å†·å´ {Config.COOLDOWN_MINUTES} åˆ†é’Ÿ", "WARN")
                    time.sleep(Config.COOLDOWN_MINUTES * 60)
            
            # åŒæ­¥å•åªè‚¡ç¥¨
            result = sync_single_stock(code, start_date, end_date)
            status = result["status"]
            stats[status] += 1
            stats["records"] += result["records"]
            
            # æ›´æ–°è¿›åº¦
            if status != "failed":
                progress["completed_codes"].append(code)
                consecutive_fails = 0
            else:
                progress["failed_codes"].append(code)
                consecutive_fails += 1
            
            progress["last_code"] = code
            
            # è¿ç»­å¤±è´¥ä¿æŠ¤
            if consecutive_fails >= Config.MAX_CONSECUTIVE_FAILS:
                log(f"ğŸ›‘ è¿ç»­å¤±è´¥ {consecutive_fails} æ¬¡ï¼Œè§¦å‘é™æµä¿æŠ¤", "WARN")
                log(f"â³ å†·å´ {Config.COOLDOWN_MINUTES} åˆ†é’Ÿåç»§ç»­...")
                save_progress(progress)
                time.sleep(Config.COOLDOWN_MINUTES * 60)
                consecutive_fails = 0
                
                # å†·å´åæ£€æŸ¥ API
                if not check_api_health():
                    log("ğŸ”´ API ä»ç„¶å—é™ï¼Œå»ºè®®ç¨åé‡è¯•", "ERROR")
                    break
            
            # å®šæœŸä¿å­˜è¿›åº¦
            if (i + 1) % 50 == 0:
                save_progress(progress)
            
            # éšæœºå»¶è¿Ÿ
            random_delay(delay, Config.DELAY_JITTER)
    
    except KeyboardInterrupt:
        log("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜è¿›åº¦...", "WARN")
    
    finally:
        save_progress(progress)
    
    # æ‰“å°ç»Ÿè®¡
    log("=" * 50)
    log(f"âœ… åŒæ­¥å®Œæˆç»Ÿè®¡:")
    log(f"   æ–°å¢: {stats['new']}")
    log(f"   æ›´æ–°: {stats['updated']}")
    log(f"   è·³è¿‡: {stats['skipped']}")
    log(f"   å¤±è´¥: {stats['failed']}")
    log(f"   æ€»è®°å½•: {stats['records']}")
    log("=" * 50)
    
    # æ›´æ–°å…ƒæ•°æ®
    metadata = {
        "last_sync_date": datetime.now().isoformat(),
        "total_stocks": len(get_synced_stocks()),
        "days": days,
        "date_range": {"start": start_date, "end": end_date},
    }
    save_metadata(metadata)
    
    # åŒæ­¥å®Œæˆï¼Œæ¸…é™¤è¿›åº¦æ–‡ä»¶
    if stats["failed"] == 0:
        clear_progress()
        log("ğŸ‰ å…¨é‡åŒæ­¥æˆåŠŸå®Œæˆï¼")


def run_incremental_sync(delay: float = 2.0):
    """å¢é‡åŒæ­¥ï¼ˆåªæ›´æ–°æœ€æ–°æ•°æ®ï¼‰"""
    ensure_dirs()
    
    log("ğŸ“Š å¼€å§‹å¢é‡åŒæ­¥...")
    
    # è·å–å·²åŒæ­¥çš„è‚¡ç¥¨
    synced_codes = get_synced_stocks()
    if not synced_codes:
        log("æ²¡æœ‰å·²åŒæ­¥çš„æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œå…¨é‡åŒæ­¥", "WARN")
        return
    
    log(f"ğŸ“Œ å·²æœ‰ {len(synced_codes)} åªè‚¡ç¥¨")
    
    # è®¡ç®—æ—¥æœŸ
    yesterday = datetime.now() - timedelta(days=1)
    end_date = yesterday.strftime("%Y%m%d")
    
    stats = {"updated": 0, "skipped": 0, "failed": 0, "records": 0}
    
    for code in tqdm(synced_codes, desc="å¢é‡æ›´æ–°"):
        # è¯»å–ç°æœ‰æ•°æ®ï¼Œç¡®å®šå¢é‡èµ·ç‚¹
        file_path = os.path.join(Config.STOCKS_DIR, f"{code}.parquet")
        try:
            existing_df = pd.read_parquet(file_path)
            existing_df['æ—¥æœŸ'] = pd.to_datetime(existing_df['æ—¥æœŸ'])
            last_date = existing_df['æ—¥æœŸ'].max()
            start_date = (last_date + timedelta(days=1)).strftime("%Y%m%d")
            
            if start_date > end_date:
                stats["skipped"] += 1
                continue
            
            result = sync_single_stock(code, start_date, end_date)
            stats[result["status"]] = stats.get(result["status"], 0) + 1
            stats["records"] += result["records"]
            
        except Exception as e:
            stats["failed"] += 1
        
        random_delay(delay, 0.5)
    
    log("=" * 50)
    log(f"âœ… å¢é‡åŒæ­¥å®Œæˆ:")
    log(f"   æ›´æ–°: {stats.get('updated', 0)}")
    log(f"   è·³è¿‡: {stats['skipped']}")
    log(f"   å¤±è´¥: {stats['failed']}")
    log(f"   æ–°å¢è®°å½•: {stats['records']}")


def run_export():
    """æ‰“åŒ…å¯¼å‡ºæ•°æ®"""
    ensure_dirs()
    
    if not os.path.exists(Config.STOCKS_DIR):
        log("æ²¡æœ‰å¯å¯¼å‡ºçš„æ•°æ®", "WARN")
        return
    
    # ç”Ÿæˆæ‰“åŒ…æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    archive_name = f"ts_share_data_{timestamp}"
    archive_path = os.path.join(Config.EXPORT_DIR, archive_name)
    
    log(f"ğŸ“¦ æ­£åœ¨æ‰“åŒ…æ•°æ®åˆ° {archive_path}.tar.gz ...")
    
    try:
        # ä½¿ç”¨ tar æ‰“åŒ…å‹ç¼©
        subprocess.run([
            "tar", "-czvf", 
            f"{archive_path}.tar.gz",
            "-C", Config.DATA_DIR,
            "stocks",
            "metadata.json"
        ], check=True, capture_output=True)
        
        # è·å–æ–‡ä»¶å¤§å°
        size_mb = os.path.getsize(f"{archive_path}.tar.gz") / (1024 * 1024)
        log(f"âœ… æ‰“åŒ…å®Œæˆ: {archive_path}.tar.gz ({size_mb:.2f} MB)")
        log(f"ğŸ’¡ ä¸‹è½½å‘½ä»¤: scp user@server:{os.path.abspath(archive_path)}.tar.gz ./")
        
    except Exception as e:
        log(f"æ‰“åŒ…å¤±è´¥: {e}", "ERROR")


def show_status():
    """æ˜¾ç¤ºåŒæ­¥çŠ¶æ€"""
    ensure_dirs()
    
    print("=" * 50)
    print("ğŸ“Š TS-Share æ•°æ®åŒæ­¥çŠ¶æ€")
    print("=" * 50)
    
    # å…ƒæ•°æ®
    metadata = load_metadata()
    synced_count = len(get_synced_stocks())
    
    print(f"\nğŸ“Œ å·²åŒæ­¥è‚¡ç¥¨: {synced_count}")
    print(f"ğŸ“… å†å²å¤©æ•°: {metadata.get('days', 'N/A')}")
    print(f"ğŸ• æœ€ååŒæ­¥: {metadata.get('last_sync_date', 'N/A')}")
    
    date_range = metadata.get('date_range', {})
    if date_range:
        print(f"ğŸ“† æ•°æ®èŒƒå›´: {date_range.get('start', '?')} ~ {date_range.get('end', '?')}")
    
    # æ•°æ®å¤§å°
    total_size = 0
    if os.path.exists(Config.STOCKS_DIR):
        for f in os.listdir(Config.STOCKS_DIR):
            if f.endswith('.parquet'):
                total_size += os.path.getsize(os.path.join(Config.STOCKS_DIR, f))
    print(f"ğŸ’¾ æ•°æ®å¤§å°: {total_size / (1024 * 1024):.2f} MB")
    
    # è¿›åº¦ä¿¡æ¯
    progress = load_progress()
    if progress.get("completed_codes"):
        print(f"\nâ³ åŒæ­¥è¿›åº¦:")
        print(f"   å·²å®Œæˆ: {len(progress['completed_codes'])}")
        print(f"   å¤±è´¥: {len(progress.get('failed_codes', []))}")
        print(f"   å¼€å§‹æ—¶é—´: {progress.get('start_time', 'N/A')}")
    
    print("=" * 50)


# ============ ä¸»å…¥å£ ============
def main():
    parser = argparse.ArgumentParser(
        description="TS-Share è¿œç¨‹æ•°æ®åŒæ­¥è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python remote_sync.py --mode full --days 120 --delay 3
  python remote_sync.py --mode incremental
  python remote_sync.py --mode export
  python remote_sync.py --mode status
        """
    )
    
    parser.add_argument(
        "--mode", 
        choices=["full", "incremental", "export", "status"],
        default="status",
        help="è¿è¡Œæ¨¡å¼: full=å…¨é‡åŒæ­¥, incremental=å¢é‡æ›´æ–°, export=æ‰“åŒ…å¯¼å‡º, status=æŸ¥çœ‹çŠ¶æ€"
    )
    
    parser.add_argument(
        "--days",
        type=int,
        default=Config.DEFAULT_DAYS,
        help=f"åŒæ­¥å¤©æ•° (é»˜è®¤: {Config.DEFAULT_DAYS})"
    )
    
    parser.add_argument(
        "--delay",
        type=float,
        default=Config.DEFAULT_DELAY,
        help=f"è¯·æ±‚é—´éš”ç§’æ•° (é»˜è®¤: {Config.DEFAULT_DELAY})"
    )
    
    parser.add_argument(
        "--no-resume",
        action="store_true",
        help="ç¦ç”¨æ–­ç‚¹ç»­ä¼ ï¼Œä»å¤´å¼€å§‹"
    )
    
    args = parser.parse_args()
    
    if args.mode == "full":
        run_full_sync(
            days=args.days, 
            delay=args.delay, 
            resume=not args.no_resume
        )
    elif args.mode == "incremental":
        run_incremental_sync(delay=args.delay)
    elif args.mode == "export":
        run_export()
    elif args.mode == "status":
        show_status()


if __name__ == "__main__":
    main()
